{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text from the given URL\n",
    "# inputï¼š\n",
    "#   url: string\n",
    "# output:\n",
    "#   c: pandas\n",
    "def URL_to_dataframe(url):\n",
    "    s = requests.get(url).content\n",
    "    c = pd.read_csv(io.StringIO(s.decode('utf-8')), header=None, delim_whitespace=True)\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh function for matrix\n",
    "# input:\n",
    "#   matrix: numpy array\n",
    "# output:\n",
    "#   numpy array\n",
    "def tanh_matrix(matrix):\n",
    "    return ((np.exp(matrix)-np.exp(-matrix))/(np.exp(matrix)+np.exp(-matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deravertive of tanh function\n",
    "# input:\n",
    "#   matrix: numpy array\n",
    "# output:\n",
    "#   numpy array\n",
    "def dertanh(matrix): \n",
    "    return (1 -np.tanh(matrix)** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class auto_encoder():\n",
    "    def __init__(self, input_dim, hidden_dim, learning_rate, constraint=False):\n",
    "        # initialize hyperparameter\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.constraint = constraint\n",
    "        \n",
    "        # initialize weight and bias\n",
    "        self.W1 = self.weight_init(1, input_dim, hidden_dim)\n",
    "        self.B1 = self.weight_init(1, 1, hidden_dim)\n",
    "        if constraint: self.W2 = np.transpose(self.W1)\n",
    "        else: self.W2 = self.weight_init(2, hidden_dim, input_dim)\n",
    "        self.B2 = self.weight_init(2, 1, input_dim)\n",
    "    \n",
    "    # output:\n",
    "    #   self.error(x[:,1:], s2)\n",
    "    #      x[:,1:]: the input feature without bias\n",
    "    #      s2: the result predicted by model\n",
    "    def forward(self, x):\n",
    "        # first layer\n",
    "        s1 = np.matmul(x, np.concatenate((self.B1, self.W1),0))        \n",
    "        x1 = np.concatenate((np.ones(shape=(s1.shape[0], 1)), np.tanh(s1)), axis=1)\n",
    "        # second layer\n",
    "        s2 = np.matmul(x1, np.concatenate((self.B2, self.W2), 0))\n",
    "        return s1, x1, s2, self.error(x[:,1:], s2)\n",
    "    \n",
    "    def train_an_epoch(self, x):\n",
    "        # forward \n",
    "        s1, x1, s2,_ = self.forward(x)\n",
    "        \n",
    "        # backward\n",
    "        # calculating delta\n",
    "        delta2 = -2 * (x[:,1:] - s2)                            # N * d\n",
    "        delta1 = np.matmul(delta2, np.transpose(self.W2)) * dertanh(s1)\n",
    "        \n",
    "        # calculating gradient\n",
    "        d_W2 = (1/x.shape[0]) * (1/self.input_dim) * np.matmul(np.transpose(x1), delta2)              # ((d~+1) * d)\n",
    "        d_W1 = (1/x.shape[0]) * (1/self.input_dim) *np.matmul(np.transpose(x), delta1)\n",
    "        \n",
    "        # update weight and bias accroding to evaluated gradient\n",
    "        self.W2 = self.W2 - self.learning_rate * d_W2[1:,:]\n",
    "        self.B2 = self.B2 - self.learning_rate * d_W2[:1,:]\n",
    "        if self.constraint:\n",
    "            self.W1 = np.transpose(self.W2) - self.learning_rate * d_W1[1:,:]\n",
    "            self.W2 = np.transpose(self.W1)\n",
    "        else:\n",
    "            self.W1 = self.W1 - self.learning_rate * d_W1[1:,:]\n",
    "        self.B1 = self.B1 - self.learning_rate * d_W1[:1,:]\n",
    "        \n",
    "        return self.error(x[:,1:], s2)\n",
    "        \n",
    "    def weight_init(self, layer_index, input_dim, output_dim):\n",
    "        U = math.sqrt(6/(1+input_dim+output_dim))\n",
    "        return np.random.uniform(-U, U, size=(input_dim, output_dim))\n",
    "    \n",
    "    # loss function\n",
    "    def error(self, x, y):  \n",
    "        diff_square = (x-y) * (x-y)\n",
    "        return (1/x.shape[0]) * ((1/x.shape[1]) * np.sum(np.sum(diff_square, axis=1),axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"http://amlbook.com/data/zip/zip.train\"\n",
    "test_url = \"http://amlbook.com/data/zip/zip.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = URL_to_dataframe(train_url)\n",
    "test_df = URL_to_dataframe(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np = train_df.to_numpy()\n",
    "test_np = test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7291, 257)\n",
      "(2007, 257)\n"
     ]
    }
   ],
   "source": [
    "print(train_np.shape)\n",
    "print(test_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np[:,0] = 1\n",
    "test_np[:,0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7291, 257)\n",
      "(2007, 257)\n"
     ]
    }
   ],
   "source": [
    "print(train_np.shape)\n",
    "print(test_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e240f0c76e04b23bc7b454436d02424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0559a543814894a35954ff8ce58472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ea2efd9f3c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mEin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_an_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#_, _, _, Eout = model.forward(test_np)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3420a66b0c30>\u001b[0m in \u001b[0;36mtrain_an_epoch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# calculating gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0md_W2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta2\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# ((d~+1) * d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0md_W1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 11, 12\n",
    "EPOCH = 5000\n",
    "log_2_hidden_layers = [1,2,3,4,5,6,7]\n",
    "\n",
    "# reserve calculated error\n",
    "Eins_11 = []\n",
    "Eouts_12 = []\n",
    "\n",
    "for log_2_hidden_layer in log_2_hidden_layers:\n",
    "    hidden_layer = 2**(log_2_hidden_layer)\n",
    "    model = auto_encoder(train_np.shape[1]-1, hidden_layer, 0.1, False)\n",
    "    #total_Ein = 0\n",
    "    #total_Eout = 0\n",
    "    \n",
    "    pbar = tqdm(total=EPOCH)\n",
    "    for epoch in range(EPOCH):\n",
    "        Ein = model.train_an_epoch(train_np)\n",
    "        \n",
    "        #_, _, _, Eout = model.forward(test_np)\n",
    "        #total_Ein += Ein\n",
    "        #total_Eout += Eout\n",
    "        pbar.update(1)\n",
    "    _, _, _, Ein = model.forward(train_np)\n",
    "    _, _, _, Eout = model.forward(test_np)\n",
    "    \n",
    "    Eins_11.append(Ein)\n",
    "    Eouts_12.append(Eout)\n",
    "    \n",
    "    #average_Eins.append(total_Ein/5000)\n",
    "    #average_Eouts.append(total_Eout/5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Eins_11)\n",
    "print(Eouts_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=100, linewidth=2)\n",
    "plt.plot(log_2_hidden_layers, Eins_11, 'o-', color='r', label=\"Ein\")\n",
    "plt.plot(log_2_hidden_layers, Eouts_12, 'o-', color='b', label=\"Eout\")\n",
    "plt.title(\"autoencoder without constraint\", x=0.5, y=1.03)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"d\", fontsize=30, labelpad=15)\n",
    "plt.ylabel(\"error\", fontsize=30, labelpad=20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13, 14\n",
    "EPOCH = 5000\n",
    "log_2_hidden_layers = [1,2,3,4,5,6,7]\n",
    "Eins_13 = []\n",
    "Eouts_14 = []\n",
    "for log_2_hidden_layer in log_2_hidden_layers:\n",
    "    hidden_layer = 2**(log_2_hidden_layer)\n",
    "    model = auto_encoder(train_np.shape[1]-1, hidden_layer, 0.1, True)\n",
    "    #total_Ein = 0\n",
    "    #total_Eout = 0\n",
    "    pbar = tqdm(total=EPOCH)\n",
    "    for epoch in range(EPOCH):\n",
    "        Ein = model.train_an_epoch(train_np)\n",
    "        #_, _, _, Eout = model.forward(test_np)\n",
    "        #total_Ein += Ein\n",
    "        #total_Eout += Eout\n",
    "        pbar.update(1)\n",
    "    \n",
    "    _, _, _, Ein = model.forward(train_np)\n",
    "    _, _, _, Eout = model.forward(test_np)\n",
    "    Eins_13.append(Ein)\n",
    "    Eouts_14.append(Eout)\n",
    "    #average_Eins.append(total_Ein/5000)\n",
    "    #average_Eouts.append(total_Eout/5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Eins_13)\n",
    "print(Eouts_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=100, linewidth=2)\n",
    "plt.plot(log_2_hidden_layers, average_Eins, 'o-', color='r', label=\"Ein\")\n",
    "plt.plot(log_2_hidden_layers, average_Eouts, 'o-', color='b', label=\"Eout\")\n",
    "plt.title(\"autoencoder with constraint\", x=0.5, y=1.03)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"d\", fontsize=30, labelpad=15)\n",
    "plt.ylabel(\"error\", fontsize=30, labelpad=20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the first feature\n",
    "X_train = np.delete(train_np, 0, 1) \n",
    "X_test = np.delete(test_np, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA():\n",
    "    def __init__(self, X, d_hidden):\n",
    "        self.X = X\n",
    "        self.d_hidden = d_hidden\n",
    "        self.x_average = (np.sum(X, 0) / X.shape[0]).reshape(1,-1)\n",
    "        self.x_normalized = X - self.x_average\n",
    "        self.w = self.w_evaluator(self.x_normalized)\n",
    "        \n",
    "    def w_evaluator(self, x_normalized):\n",
    "        XtX = np.matmul(np.transpose(x_normalized), x_normalized)\n",
    "        eigen_value, eigen_vector = LA.eig(XtX)\n",
    "        eigen_value_sort = np.argsort(-eigen_value)\n",
    "        sorted_eigen_vector = eigen_vector[:,eigen_value_sort]\n",
    "        d_hidden_eigen_vector = sorted_eigen_vector[:,:self.d_hidden]\n",
    "        \n",
    "        return d_hidden_eigen_vector\n",
    "    \n",
    "    def linear_autoencoder_train(self):\n",
    "        return np.matmul(np.matmul(self.x_normalized, self.w), np.transpose(self.w)) + self.x_average\n",
    "    \n",
    "    def loss_train(self):\n",
    "        transfered_x = self.linear_autoencoder_train()\n",
    "        squre_diff = (self.X - transfered_x) * (self.X - transfered_x)\n",
    "        return (1/self.X.shape[0]) * (1/self.X.shape[1]) * np.sum(np.sum(squre_diff, 1), 0)\n",
    "    \n",
    "    def linear_autoencoder_test(self, X_test):\n",
    "        X_test_average = (np.sum(X_test, 0) / X_test.shape[0]).reshape(1,-1)\n",
    "        X_test_normalized = X_test - X_test_average\n",
    "        return np.matmul(np.matmul(X_test_normalized, self.w), np.transpose(self.w)) + X_test_average\n",
    "    \n",
    "    def loss_test(self, X_test):\n",
    "        transfered_x_test = self.linear_autoencoder_test(X_test)\n",
    "        squre_diff = (X_test - transfered_x_test) * (X_test - transfered_x_test)\n",
    "        return (1/X_test.shape[0]) * (1/X_test.shape[1]) * np.sum(np.sum(squre_diff, 1), 0)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PCA(X_train, 8)\n",
    "print(model.loss_train())\n",
    "print(model.loss_test(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15, 16\n",
    "log_2_hidden_layers = [1,2,3,4,5,6,7]\n",
    "Eins_15 = []\n",
    "Eouts_16 = []\n",
    "for log_2_hidden_layer in log_2_hidden_layers:\n",
    "    hidden_layer = 2**(log_2_hidden_layer)\n",
    "    model = PCA(X_train, hidden_layer)\n",
    "    \n",
    "    Eins_15.append(model.loss_train())\n",
    "    Eouts_16.append(model.loss_test(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Eins_15)\n",
    "print(Eouts_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=100, linewidth=2)\n",
    "plt.plot(log_2_hidden_layers, Eins, 'o-', color='r', label=\"Ein\")\n",
    "plt.plot(log_2_hidden_layers, Eouts, 'o-', color='b', label=\"Eout\")\n",
    "plt.title(\"PCA\", x=0.5, y=1.03)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"d\", fontsize=30, labelpad=15)\n",
    "plt.ylabel(\"error\", fontsize=30, labelpad=20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "plt.figure(figsize=(15, 10), dpi=100, linewidth=2)\n",
    "plt.plot(log_2_hidden_layers, Eins_11, 'o-', color='r', label=\"Ein_11\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"$log2\\widetilde{d}$\", fontsize=30, labelpad=15)\n",
    "plt.ylabel(\"error\", fontsize=30, labelpad=20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12\n",
    "plt.figure(figsize=(15, 10), dpi=100, linewidth=2)\n",
    "plt.plot(log_2_hidden_layers, Eouts_12, 'o-', color='b', label=\"Eout_12\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"$log2\\widetilde{d}$\", fontsize=30, labelpad=15)\n",
    "plt.ylabel(\"error\", fontsize=30, labelpad=20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13\n",
    "plt.figure(figsize=(15, 10), dpi=100, linewidth=2)\n",
    "plt.plot(log_2_hidden_layers, Eins_11, 'o-', color='b', label=\"Eins_11\")\n",
    "plt.plot(log_2_hidden_layers, Eins_13, 'o-', color='r', label=\"Eins_13\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"$log2\\widetilde{d}$\", fontsize=30, labelpad=15)\n",
    "plt.ylabel(\"error\", fontsize=30, labelpad=20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14\n",
    "plt.figure(figsize=(15, 10), dpi=100, linewidth=2)\n",
    "plt.plot(log_2_hidden_layers, Eouts_12, 'o-', color='b', label=\"Eouts_12\")\n",
    "plt.plot(log_2_hidden_layers, Eouts_14, 'o-', color='r', label=\"Eouts_14\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"$log2\\widetilde{d}$\", fontsize=30, labelpad=15)\n",
    "plt.ylabel(\"error\", fontsize=30, labelpad=20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 15\n",
    "plt.figure(figsize=(15, 10), dpi=100, linewidth=2)\n",
    "plt.plot(log_2_hidden_layers, Eins_13, 'o-', color='b', label=\"Eins_13\")\n",
    "plt.plot(log_2_hidden_layers, Eins_15, 'o-', color='r', label=\"Eins_15\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"$log2\\widetilde{d}$\", fontsize=30, labelpad=15)\n",
    "plt.ylabel(\"error\", fontsize=30, labelpad=20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16\n",
    "plt.figure(figsize=(15, 10), dpi=100, linewidth=2)\n",
    "plt.plot(log_2_hidden_layers, Eouts_14, 'o-', color='b', label=\"Eouts_14\")\n",
    "plt.plot(log_2_hidden_layers, Eouts_16, 'o-', color='r', label=\"Eouts_16\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"$log2\\widetilde{d}$\", fontsize=30, labelpad=15)\n",
    "#plt.xlabel(\"d\", fontsize=30, labelpad=15)\n",
    "plt.ylabel(\"error\", fontsize=30, labelpad=20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
